{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 3: Searching for Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this mini project, I'll be using the following:\n",
    "#### Elasticsearch\n",
    "#### Tweepy\n",
    "#### json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch import Elasticsearch                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_key = 'wwwKNtSMuTHqgL0KLp9RFqySu'\n",
    "secret_c_key = 'ekkrXaxm3zOfGSIoQuMckt3I3ig5HBNw8uZgZYLgswASV8xWqP'\n",
    "access_token = '846115336877432832-M7UoANTmDy2uzOAHVN8WgIuOnSd7L6g'\n",
    "secret_access_token = 'Qi7fahFkg9LfM3PRLurqXZkmcxNWgCHmKz39Jn4gTdFTK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These keys and tokens will probably be changed by the time I upload the nbviewr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"elasticsearch\").setLevel(logging.CRITICAL)\n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_HOST = {\"host\": \"localhost\", \"port\": 9200}\n",
    "elast = Elasticsearch(hosts=[ES_HOST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Marruecos67: Rajoute un t-max aussi https://t.co/oqFMP5BPcd\n",
      "NO FAIR!!! whiting out the board. NO FAIR!!!\n",
      "RT @cpeckk: i recently discovered that i can get chuck e cheese pizza delivered to my house via postmates and i really just want to know wh…\n",
      "Para que no me vaya de mi casa mi mamá me compra pizza . Que trucazo\n",
      "RT @Jiminpicss: friendly reminder that jimin bites his ice cream, puts milk before cereal and likes pineapple on his pizza.\n",
      "YES YES YES\n",
      "The target Reuben pizza bowl\n",
      "RT @imajsaclaimant: This is great... Khan Pizza takeaway in Hull to open its doors on Christmas Day to give out essential items and hot foo…\n",
      "RT @pizzahutuk: Hi @ManUtd, we've received a CV from Mr J Mourinho. \n",
      "He's just applied for a job as pizza chef.\n",
      "Please can you send through…\n",
      "@imnayyaeon Mau peje pizza sm matcha :(\n",
      "Open NOW until Friday night for \"OVERTIME! Customer Appreciation Days!!\". Hit the SEND MESSAGE button or call/text… https://t.co/q7Lp6jvpXF\n",
      "RT @pizzahutuk: Hi @ManUtd, we've received a CV from Mr J Mourinho. \n",
      "He's just applied for a job as pizza chef.\n",
      "Please can you send through…\n",
      "I liked a @YouTube video https://t.co/WBXdtu3F6V My first Pompeii Brick Pizza Oven - Time Lapse Video\n",
      "RT @TheEvansPosts: girls will go to the bar sober with $4 and some sideboob and go home completely lit with $4 and a box of pizza, masters…\n",
      "RT @jeperego: Ossignur. @ginosorbillo https://t.co/92TP7OAWRt\n"
     ]
    }
   ],
   "source": [
    "class Listener(StreamListener):\n",
    "    def __init__(self, number):\n",
    "        self.counter = 0\n",
    "        self.number = number\n",
    "\n",
    "    def on_data(self, data):\n",
    "        dict_data = json.loads(data)\n",
    "        print (dict_data[\"text\"])\n",
    "        elast.index(index=\"twitter\",\n",
    "                 doc_type=\"stream\",\n",
    "                 body={\"author\": dict_data[\"user\"][\"screen_name\"],\n",
    "                       \"date\": dict_data[\"created_at\"],\n",
    "                       \"message\": dict_data[\"text\"]})\n",
    "        self.counter +=1\n",
    "        if self.counter == self.number:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "                       \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    Tweet_Listening = Listener(number = 15) \n",
    "    Auth = OAuthHandler(c_key, secret_c_key)\n",
    "    Auth.set_access_token(access_token, secret_access_token)\n",
    "    Streaming = Stream(Auth, Tweet_Listening)\n",
    "    Streaming.filter(track=['pizza', 'hamburger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fun part is that whenever I want to execute another search, I have to uninstall and reinstall Elasticsearch. I'm sure there's some file or folder I could delete that would have the same effect, but I don't want to spend hours testing which one that would be. Regardless, I have my tweets, and I'm happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This also explains why there is a huge gap between the numbers for the Elasticsearch part and the query part, I don't want to restart my Python3 code because I'd have to reinstall Elasticsearch. Anyway, for this next line, I just wanted to explore if Elastisearch created nodes with the indices I set for it earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".kibana_1\n",
      "twitter\n"
     ]
    }
   ],
   "source": [
    "for index in elast.indices.get('*'):\n",
    "  print (index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the search itself: to find the tweets in our Elasticsearch index that do not have a source field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 2, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': 15, 'max_score': 1.0, 'hits': [{'_index': 'twitter', '_type': 'stream', '_id': '1VZzw2cBcHlmTvfkD-dd', '_score': 1.0, '_source': {'author': 'noregre73766334', 'date': 'Tue Dec 18 22:32:31 +0000 2018', 'message': 'RT @Marruecos67: Rajoute un t-max aussi https://t.co/oqFMP5BPcd'}}, {'_index': 'twitter', '_type': 'stream', '_id': '2VZzw2cBcHlmTvfkFedp', '_score': 1.0, '_source': {'author': 'joonbootie', 'date': 'Tue Dec 18 22:32:33 +0000 2018', 'message': 'RT @Jiminpicss: friendly reminder that jimin bites his ice cream, puts milk before cereal and likes pineapple on his pizza.'}}, {'_index': 'twitter', '_type': 'stream', '_id': '4VZzw2cBcHlmTvfkKOcM', '_score': 1.0, '_source': {'author': 'Carmdee66Carman', 'date': 'Tue Dec 18 22:32:38 +0000 2018', 'message': 'I liked a @YouTube video https://t.co/WBXdtu3F6V My first Pompeii Brick Pizza Oven - Time Lapse Video'}}, {'_index': 'twitter', '_type': 'stream', '_id': '21Zzw2cBcHlmTvfkFufP', '_score': 1.0, '_source': {'author': 'psucello13', 'date': 'Tue Dec 18 22:32:33 +0000 2018', 'message': 'The target Reuben pizza bowl'}}, {'_index': 'twitter', '_type': 'stream', '_id': '4FZzw2cBcHlmTvfkJ-dY', '_score': 1.0, '_source': {'author': 'gladglads', 'date': 'Tue Dec 18 22:32:37 +0000 2018', 'message': \"RT @pizzahutuk: Hi @ManUtd, we've received a CV from Mr J Mourinho. \\nHe's just applied for a job as pizza chef.\\nPlease can you send through…\"}}, {'_index': 'twitter', '_type': 'stream', '_id': '41Zzw2cBcHlmTvfkLeeV', '_score': 1.0, '_source': {'author': 'r50', 'date': 'Tue Dec 18 22:32:39 +0000 2018', 'message': 'RT @jeperego: Ossignur. @ginosorbillo https://t.co/92TP7OAWRt'}}, {'_index': 'twitter', '_type': 'stream', '_id': '1lZzw2cBcHlmTvfkEOc9', '_score': 1.0, '_source': {'author': 'BrianLeeOkert', 'date': 'Tue Dec 18 22:32:31 +0000 2018', 'message': 'NO FAIR!!! whiting out the board. NO FAIR!!!'}}, {'_index': 'twitter', '_type': 'stream', '_id': '3FZzw2cBcHlmTvfkGudY', '_score': 1.0, '_source': {'author': 'SocialistNutter', 'date': 'Tue Dec 18 22:32:34 +0000 2018', 'message': 'RT @imajsaclaimant: This is great... Khan Pizza takeaway in Hull to open its doors on Christmas Day to give out essential items and hot foo…'}}, {'_index': 'twitter', '_type': 'stream', '_id': '11Zzw2cBcHlmTvfkEueC', '_score': 1.0, '_source': {'author': 'DesireeSchultzz', 'date': 'Tue Dec 18 22:32:32 +0000 2018', 'message': 'RT @cpeckk: i recently discovered that i can get chuck e cheese pizza delivered to my house via postmates and i really just want to know wh…'}}, {'_index': 'twitter', '_type': 'stream', '_id': '2FZzw2cBcHlmTvfkFOfM', '_score': 1.0, '_source': {'author': 'agus__enrico', 'date': 'Tue Dec 18 22:32:33 +0000 2018', 'message': 'Para que no me vaya de mi casa mi mamá me compra pizza . Que trucazo'}}]}}\n"
     ]
    }
   ],
   "source": [
    "res = elast.search(index = \"twitter\", doc_type = \"stream\", _source = True)\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So out of the tweets we gathered, all of them appear here. We can still narrow this down by adding in one of the parameters listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 2, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': 1, 'max_score': 1.0, 'hits': [{'_index': 'twitter', '_type': 'stream', '_id': '1VZzw2cBcHlmTvfkD-dd', '_score': 1.0, '_source': {'author': 'noregre73766334', 'date': 'Tue Dec 18 22:32:31 +0000 2018', 'message': 'RT @Marruecos67: Rajoute un t-max aussi https://t.co/oqFMP5BPcd'}}]}}\n"
     ]
    }
   ],
   "source": [
    "res2 = elast.search(index = \"twitter\", doc_type = \"stream\", body={\"query\": {\"match\": {\"_id\":\"1VZzw2cBcHlmTvfkD-dd\"}}})\n",
    "print (res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've narrowed down the tweets form our Elasticsearch module to one with a specific ID through the search command here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
